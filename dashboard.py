import streamlit as st
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Streamlit Cloudìš© ë°±ì—”ë“œ ì„¤ì •
import matplotlib.pyplot as plt
from src.pipeline import load_data, classify_posts, ewma_anomaly_detection
from src.explain import word_importance
from src.config import DEFAULT_THRESHOLD, ALPHA

# -----------------------------------
# Page setup
# -----------------------------------
st.set_page_config(page_title="Game Board Monitor PoC", layout="wide")
st.title("ğŸ•¹ï¸ Game Board Monitor PoC (3.1 ~ 3.4)")

st.markdown("""
ë³¸ ë°ëª¨ëŠ” ë‹¤ìŒ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

1) **ë°ì´í„° ìˆ˜ì§‘/ì „ì²˜ë¦¬**: CSV ì—…ë¡œë“œ ë˜ëŠ” ìƒ˜í”Œ ì‚¬ìš©  
2) **ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜**: ì¹´í…Œê³ ë¦¬(ê·œì¹™ ê¸°ë°˜) + ê°ì •(KoELECTRA)  
3) **ì„¤ëª… ê°€ëŠ¥ì„±(ê°„ì´)**: í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë‹¨ì–´ ì¤‘ìš”ë„ í•˜ì´ë¼ì´íŠ¸  
4) **EWMA ì´ìƒì¹˜ íƒì§€**: ì‹œê°„ ë²„í‚·ë³„ ì´ìŠˆ ë¹ˆë„ ê¸°ë°˜ ê²½ë³´  
""")

# -----------------------------------
# 3.1 ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬
# -----------------------------------
SAMPLE_PATH = "data/sample_game_posts_100_realistic.csv"
st.subheader("â‘  ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
st.caption(f"ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© ì¤‘: {SAMPLE_PATH}")
df = load_data(SAMPLE_PATH)


# -----------------------------------
# 3.2 ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
# -----------------------------------
st.subheader("â‘¡ ë¶„ë¥˜ ì‹¤í–‰ (í•˜ì´ë¸Œë¦¬ë“œ: ê·œì¹™ + KoELECTRA)")

st.info("""
**ğŸŒŸ KoELECTRA + ê·œì¹™ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ (Fine-tuned)**

- **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜** (ë¡œê·¸ì¸, ê²°ì œ, ë ‰ ë“±): ê·œì¹™ ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­  
- **ê°ì • ë¶„ì„** (ë¶€ì •/ì¤‘ë¦½/ê¸ì •): **Fine-tuned KoELECTRA** ëª¨ë¸ ì‚¬ìš©  
- **ì´ìŠˆ íŒë‹¨**: ë¶€ì • ê°ì • + íŠ¹ì • ì¹´í…Œê³ ë¦¬ â†’ ì´ìŠˆ

ğŸ“Š **ê²Œì„ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„°ë¡œ Fine-tuning ì™„ë£Œ** (Validation ì •í™•ë„: 100%)  
ğŸ¯ í”„ë¡œì íŠ¸ ì‚¬ì „ë³´ê³ ì„œì— ëª…ì‹œëœ **KoELECTRAë¥¼ ì‹¤ì œ ì ìš© ë° ìµœì í™”**í•œ êµ¬í˜„ì…ë‹ˆë‹¤.
""")

# classify_posts()ëŠ” ê° ê²Œì‹œê¸€(text)ì— ëŒ€í•´ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
# 
# ğŸ”¹ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜: config.pyì˜ ISSUE_CATEGORIES í‚¤ì›Œë“œë¡œ ê·œì¹™ ê¸°ë°˜ ë§¤ì¹­
# ğŸ”¹ ê°ì • ë¶„ì„: KoELECTRA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¶€ì •/ì¤‘ë¦½/ê¸ì • ì˜ˆì¸¡
#
# í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ì—ëŠ” ë‹¤ìŒ 3ê°œì˜ ì£¼ìš” ì»¬ëŸ¼ì´ ì¶”ê°€ë©ë‹ˆë‹¤:
#
#   1ï¸âƒ£ pred_categories : íƒì§€ëœ ë¬¸ì œ ìœ í˜•(category) ë¦¬ìŠ¤íŠ¸
#        ì˜ˆ) ["ë¡œê·¸ì¸"], ["ê²°ì œ"], ["ì¼ë°˜"]
#        - "ì¼ë°˜"ì€ ì¥ì•  ê´€ë ¨ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì€ ê²½ìš°
#
#   2ï¸âƒ£ pred_sentiment : ë¬¸ì¥ì˜ ê°ì • ê²½í–¥ (KoELECTRA ì˜ˆì¸¡)
#        ì˜ˆ) "ë¶€ì •", "ì¤‘ë¦½", "ê¸ì •"
#        - KoELECTRA ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ê°ì •ì„ ì˜ˆì¸¡
#        - ì‚¬ì „í•™ìŠµëœ í•œêµ­ì–´ ELECTRA ëª¨ë¸ í™œìš©
#
#   3ï¸âƒ£ is_issue : ì‹¤ì œ 'ì´ìŠˆ ê²Œì‹œê¸€'ë¡œ ê°„ì£¼ë˜ëŠ”ì§€ ì—¬ë¶€ (True/False)
#        - ì¡°ê±´: pred_sentiment == "ë¶€ì •" ì´ë©´ì„œ pred_categories != ["ì¼ë°˜"]
#        - ì¦‰, ë¶€ì •ì ì¸ ê°ì •ì„ ê°€ì§„ ì¥ì•  ê´€ë ¨ ê²Œì‹œê¸€ë§Œ Trueë¡œ í‘œì‹œ
#
# ì´ë ‡ê²Œ ë¶„ë¥˜ëœ ë°ì´í„°ëŠ” ì´í›„ EWMA ì´ìƒì¹˜ íƒì§€(3.3) ë‹¨ê³„ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
# ë¶€ì • ê²Œì‹œê¸€ì´ ì‹œê°„ëŒ€ë³„ë¡œ ì–¼ë§ˆë‚˜ ë°œìƒí–ˆëŠ”ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìƒ íŒ¨í„´ì„ ê°ì§€í•˜ê²Œ ë©ë‹ˆë‹¤.


pred_df = classify_posts(df)

# ë°©ì–´: is_issue ë³´ì¥ (í˜¹ì‹œ ë‹¤ë¥¸ ë²„ì „ì˜ classify_postsë¥¼ ì“°ë”ë¼ë„ ì•ˆì „í•˜ê²Œ)
if "is_issue" not in pred_df.columns:
    if {"pred_categories", "pred_sentiment"}.issubset(pred_df.columns):
        pred_df["is_issue"] = pred_df["pred_sentiment"].eq("ë¶€ì •") & \
                              pred_df["pred_categories"].apply(lambda x: x != ["ì¼ë°˜"])
    else:
        st.error("ë¶„ë¥˜ ê²°ê³¼ì— í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤. 'classify_posts()' êµ¬í˜„ì„ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

# í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ í•¨ìˆ˜
def extract_key_words(text):
    """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (ê°„ëµ í‘œì‹œìš©)"""
    from src.explain import word_importance
    tokens, scores = word_importance(text)
    
    # ì ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 3ê°œ ë‹¨ì–´ë§Œ
    word_scores = [(t, s) for t, s in zip(tokens, scores) if s > 0]
    word_scores.sort(key=lambda x: x[1], reverse=True)
    
    if not word_scores:
        return "-"
    
    # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
    top_words = word_scores[:3]
    result = ", ".join([f"{w}({s:.1f})" for w, s in top_words])
    return result

# í•µì‹¬ ë‹¨ì–´ ì»¬ëŸ¼ ì¶”ê°€
pred_df["í•µì‹¬_ë‹¨ì–´"] = pred_df["text"].apply(extract_key_words)

# ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (í•µì‹¬ ë‹¨ì–´ í¬í•¨)
st.dataframe(
    pred_df[["id", "title", "content", "pred_categories", "pred_sentiment", "í•µì‹¬_ë‹¨ì–´", "is_issue", "date"]],
    use_container_width=True
)

# -----------------------------------
# ğŸ’¾ ë¶„ë¥˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì¶”ê°€
# -----------------------------------
import io

# CSV ë²„ì „ (ì„ íƒ)
st.download_button(
    label="â‘¡ ë¶„ë¥˜ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
    data=pred_df.to_csv(index=False).encode("utf-8-sig"),
    file_name="classification_results.csv",
    mime="text/csv",
)

# -----------------------------------
# 3.4 ì„¤ëª… ê°€ëŠ¥ì„± (í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê¸°ë°˜)
# -----------------------------------
st.subheader("â‘¢ ë¶„ë¥˜ ê·¼ê±° í™•ì¸ (ì„¤ëª… ê°€ëŠ¥ì„±)")

st.markdown("""
**ğŸ’¡ ì™œ ì´ ê²Œì‹œê¸€ì´ "ë¶€ì •"ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆë‚˜ìš”?**

ì•„ë˜ì—ì„œ í–‰ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ë©´, ì–´ë–¤ ë‹¨ì–´ê°€ ë¶„ë¥˜ì— ì˜í–¥ì„ ì£¼ì—ˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")

# ì´ ë‹¨ê³„ëŠ” ê° ê²Œì‹œê¸€ì˜ ë¬¸ì¥ ë‚´ì—ì„œ ì–´ë–¤ ë‹¨ì–´ê°€ 'ì´ìŠˆ íƒì§€'ì— ì˜í–¥ì„ ì£¼ì—ˆëŠ”ì§€ë¥¼
# ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ê°„ë‹¨í•œ Explainability(ì„¤ëª… ê°€ëŠ¥ì„±) ëª¨ë“ˆì…ë‹ˆë‹¤.
#
# ë‚´ë¶€ì ìœ¼ë¡œ src/explain.pyì˜ `word_importance()` í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ë©°,
# ê·¸ í•¨ìˆ˜ëŠ” ë‹¤ìŒ ê·œì¹™ìœ¼ë¡œ ê° ë‹¨ì–´ì— ì ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤:
#
#   1) ISSUE_CATEGORIESì— í¬í•¨ëœ ì¥ì•  í‚¤ì›Œë“œ ë°œê²¬ ì‹œ +1.0
#   2) NEGATIVE_CUES(ë¶€ì • ë‹¨ì„œ)ê°€ í¬í•¨ë˜ë©´ +0.5
#   3) POSITIVE_CUES(ê¸ì • ë‹¨ì„œ)ê°€ í¬í•¨ë˜ë©´ +0.3
#
# ì´í›„ ì ìˆ˜ë¥¼ [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”(normalization)í•˜ì—¬
# Streamlit í™”ë©´ì—ì„œ ë‹¨ì–´ë³„ë¡œ ë¹¨ê°„ ìŒì˜(red heatmap intensity)ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.
#
#  â€¢ ì§„í•˜ê²Œ í‘œì‹œëœ ë‹¨ì–´ì¼ìˆ˜ë¡ ëª¨ë¸(í˜¹ì€ ê·œì¹™)ì´ 'ì´ ë¬¸ì¥ì€ ë¬¸ì œ ìƒí™©ì´ë‹¤'ë¼ê³ 
#    íŒë‹¨í•˜ëŠ” ë° ê¸°ì—¬í•œ ë¹„ì¤‘ì´ í¼
#  â€¢ ì´ PoCì—ì„œëŠ” ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë°©ì‹ì´ì§€ë§Œ,
#    ì‹¤ì œ ìš´ì˜í™˜ê²½ì—ì„œëŠ” LRP(ë ˆì´ì–´ë³„ ê´€ë ¨ì„± ì „íŒŒ, Layer-wise Relevance Propagation),
#    Integrated Gradients ë“±ì˜ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ë°©ë²•ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥


row_ix = st.number_input("ì„¤ëª…ì„ ë³¼ í–‰ index ì„ íƒ", min_value=0, max_value=len(pred_df)-1, value=0, step=1)
text = pred_df.iloc[int(row_ix)]["text"]
tokens, scores = word_importance(text)

def colorize_enhanced(tokens, scores):
    """ê°œì„ ëœ ì‹œê°í™”: ìƒ‰ìƒ + ë ˆì´ë¸” + ì ìˆ˜"""
    from src.config import ISSUE_CATEGORIES, NEGATIVE_CUES, POSITIVE_CUES
    
    html = []
    for t, s in zip(tokens, scores):
        # ë‹¨ì–´ ì¢…ë¥˜ íŒë‹¨
        word_type = ""
        emoji = ""
        color = "red"
        
        # ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì²´í¬
        is_category = False
        for cat, keywords in ISSUE_CATEGORIES.items():
            if any(kw in t for kw in keywords):
                word_type = f"[{cat}]"
                emoji = "ğŸ¯"
                color = "red"
                is_category = True
                break
        
        # ë¶€ì •ì–´ ì²´í¬
        if not is_category and any(cue in t for cue in NEGATIVE_CUES):
            word_type = "[ë¶€ì •]"
            emoji = "âš ï¸"
            color = "orange"
        
        # ê¸ì •ì–´ ì²´í¬
        elif not is_category and any(cue in t for cue in POSITIVE_CUES):
            word_type = "[ê¸ì •]"
            emoji = "âœ…"
            color = "green"
        
        # HTML ìƒì„±
        if s > 0:  # ì ìˆ˜ê°€ ìˆëŠ” ë‹¨ì–´ë§Œ ê°•ì¡°
            html.append(
                f"<span style='background-color: rgba({'255,0,0' if color=='red' else '255,165,0' if color=='orange' else '0,200,0'},{s}); "
                f"padding:4px 8px; border-radius:5px; margin:2px; display:inline-block; "
                f"border: 2px solid {color};'>"
                f"{emoji} <strong>{t}</strong> "
                f"<small style='opacity:0.8;'>{word_type} {s:.1f}</small>"
                f"</span>"
            )
        else:
            html.append(f"<span style='margin:2px;'>{t}</span>")
    
    return " ".join(html)

def create_word_table(tokens, scores):
    """ë‹¨ì–´ ë¶„ì„ í…Œì´ë¸” ìƒì„±"""
    from src.config import ISSUE_CATEGORIES, NEGATIVE_CUES, POSITIVE_CUES
    import pandas as pd
    
    data = []
    for t, s in zip(tokens, scores):
        if s > 0:  # ì ìˆ˜ê°€ ìˆëŠ” ë‹¨ì–´ë§Œ
            word_type = "ì¤‘ë¦½"
            reason = "-"
            
            # ì¹´í…Œê³ ë¦¬ ì²´í¬
            for cat, keywords in ISSUE_CATEGORIES.items():
                if any(kw in t for kw in keywords):
                    word_type = f"ì¹´í…Œê³ ë¦¬({cat})"
                    reason = f"'{cat}' ë¬¸ì œ í‚¤ì›Œë“œ"
                    break
            
            # ë¶€ì •ì–´ ì²´í¬
            if word_type == "ì¤‘ë¦½" and any(cue in t for cue in NEGATIVE_CUES):
                word_type = "ë¶€ì •ì–´"
                reason = "ë¶€ì •ì  í‘œí˜„"
            
            # ê¸ì •ì–´ ì²´í¬
            elif word_type == "ì¤‘ë¦½" and any(cue in t for cue in POSITIVE_CUES):
                word_type = "ê¸ì •ì–´"
                reason = "ê¸ì •ì  í‘œí˜„"
            
            data.append({
                "ë‹¨ì–´": t,
                "ì¢…ë¥˜": word_type,
                "ì¤‘ìš”ë„": f"{s:.2f}",
                "ì´ìœ ": reason
            })
    
    if data:
        return pd.DataFrame(data)
    return None

st.markdown("**ì›ë¬¸ í…ìŠ¤íŠ¸**")
st.write(text)

# ğŸ“˜ "ê°„ì´ LRP ëŒ€ì²´"ë€?
#
#  â€¢ LRP(Layer-wise Relevance Propagation)ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸(BERT, KoELECTRA ë“±)ì˜
#    ì˜ˆì¸¡ ê²°ê³¼ê°€ ì…ë ¥ì˜ ì–´ë–¤ ë‹¨ì–´(ë˜ëŠ” íŠ¹ì§•)ì—ì„œ ê¸°ì¸í–ˆëŠ”ì§€ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì—­ì¶”ì í•˜ëŠ”
#    ì„¤ëª… ê°€ëŠ¥í•œ AI(Explainable AI) ê¸°ë²•ì´ë‹¤.
#
#  â€¢ ë³¸ PoCëŠ” ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ë°©ì‹ìœ¼ë¡œ, ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜(weight)ê°€ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—
#    LRP ê°™ì€ ìˆ˜í•™ì  ì—­ì „íŒŒ ë°©ì‹ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.
#
#  â€¢ ëŒ€ì‹  ë‹¨ì–´ë³„ë¡œ ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜ ê·œì¹™ì„ ì ìš©í•´ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê³ ,
#    ì´ë¥¼ ìƒ‰ìƒ ê°•ë„ë¡œ ì‹œê°í™”í•˜ì—¬ "ë”¥ëŸ¬ë‹ì˜ LRP ê²°ê³¼ì²˜ëŸ¼ ë³´ì´ëŠ”"
#    ê°„ì´ í˜•íƒœì˜ ì„¤ëª… ëª¨ë“ˆë¡œ êµ¬í˜„í•˜ì˜€ë‹¤.
#
#  â€¢ ì¦‰, "ê°„ì´ LRP ëŒ€ì²´"ë€ ë³µì¡í•œ ë”¥ëŸ¬ë‹ ì„¤ëª… ê¸°ë²•ì„
#    ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°ìœ¼ë¡œ ë‹¨ìˆœ ëŒ€ì²´í•œ ë²„ì „ì„.
#    (ëª©ì : PoC ë‹¨ê³„ì—ì„œ ì„¤ëª… ê°€ëŠ¥ì„±(Explainability) ê°œë…ì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸)

st.markdown("**ğŸ¨ ë‹¨ì–´ ì¤‘ìš”ë„ ì‹œê°í™” (ê°œì„ )**", unsafe_allow_html=True)
st.markdown(colorize_enhanced(tokens, scores), unsafe_allow_html=True)

# ìƒì„¸ ë¶„ì„ í…Œì´ë¸” ì¶”ê°€
st.markdown("**ğŸ“Š ìƒì„¸ ë‹¨ì–´ ë¶„ì„**")
word_table = create_word_table(tokens, scores)
if word_table is not None:
    st.dataframe(word_table, use_container_width=True)
else:
    st.info("ì¤‘ìš” ë‹¨ì–´ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# -----------------------------------
# 3.3 EWMA ì´ìƒì¹˜ íƒì§€
# -----------------------------------
st.subheader("â‘£ EWMA ì´ìƒì¹˜ íƒì§€")

# EWMA(Exponentially Weighted Moving Average)ëŠ” ì‹œê°„ íë¦„ì— ë”°ë¼
# ìµœì‹  ë°ì´í„°ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” 'ì§€ìˆ˜ê°€ì¤‘ ì´ë™í‰ê· 'ì…ë‹ˆë‹¤.
# Î±(alpha)ëŠ” EWMAì˜ "ë¯¼ê°ë„"ë¥¼ ì¡°ì ˆí•˜ëŠ” ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤.
#
#   â€¢ Î± â†“ (ì˜ˆ: 0.1~0.3) â†’ ì˜¤ë˜ëœ ë°ì´í„°ë„ ë°˜ì˜ â†’ ê³¡ì„ ì´ ë¶€ë“œëŸ½ê³  ì•ˆì •ì 
#                       â†’ ë³€í™”ì— ë‘”ê°í•˜ì§€ë§Œ ë…¸ì´ì¦ˆì— ê°•í•¨
#   â€¢ Î± â†‘ (ì˜ˆ: 0.5~0.9) â†’ ìµœì‹  ë°ì´í„° ìœ„ì£¼ ë°˜ì˜ â†’ ê³¡ì„ ì´ ë¯¼ê°í•˜ê²Œ ë³€ë™
#                       â†’ ë³€í™”ì— ì¦‰ê° ë°˜ì‘í•˜ì§€ë§Œ ê±°ì§“ ê²½ë³´ ë§ìŒ
#
# z-threshold(ì„ê³„ì¹˜)ëŠ” ê²½ë³´ ë°œìƒ ê¸°ì¤€ì´ ë˜ëŠ” z-score(í‘œì¤€í¸ì°¨ ë°°ìˆ˜) ê°’ì…ë‹ˆë‹¤.
#   â€¢ ì„ê³„ì¹˜ â†“ (ì˜ˆ: 1.5) â†’ ì‘ì€ ë³€í™”ë„ ê²½ë³´ë¡œ ê°ì§€ (ë¯¼ê°)
#   â€¢ ì„ê³„ì¹˜ â†‘ (ì˜ˆ: 3.0) â†’ í° ë³€í™”ë§Œ ê²½ë³´ë¡œ ê°ì§€ (ë³´ìˆ˜ì )
#
# ë”°ë¼ì„œ ë‘ ê°’ì„ ì¡°í•©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ë™ì‘ íŠ¹ì„±ì„ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
#   Î±=0.4~0.6, ì„ê³„ì¹˜=2.0 â†’ ê¶Œì¥ê°’ (ì ë‹¹í•œ ë¯¼ê°ë„ì™€ ì•ˆì •ì„±)
#   Î± ë†’ìŒ + ì„ê³„ì¹˜ ë‚®ìŒ  â†’ ë§¤ìš° ë¹ ë¥´ì§€ë§Œ ê±°ì§“ ê²½ë³´ ë§ìŒ
#   Î± ë‚®ìŒ + ì„ê³„ì¹˜ ë†’ìŒ  â†’ ëŠë¦¬ì§€ë§Œ ì‹ ë¢°ë„ ë†’ì€ ê²½ë³´ë§Œ íƒì§€

freq = st.selectbox("ì§‘ê³„ ì£¼ê¸°", ["5min", "10min", "15min", "30min", "1H"], index=2)
alpha = st.slider("EWMA Î± (smoothing)", 0.05, 0.9, ALPHA, 0.05)
zth = st.slider("ì„ê³„ì¹˜ (|z| â‰¥ ì„ê³„ì¹˜ ì‹œ ê²½ë³´)", 1.0, 5.0, DEFAULT_THRESHOLD, 0.5)

try:
    an = ewma_anomaly_detection(pred_df, freq=freq, alpha=alpha, z_thresh=zth)
except Exception as e:
    st.error(f"ì´ìƒì¹˜ íƒì§€ ì¤‘ ì˜¤ë¥˜: {e}")
    st.stop()

if an.empty:
    st.warning("ì§‘ê³„ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ê°„/ë²„í‚· ì„¤ì • ë˜ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
else:
    fig, ax = plt.subplots(figsize=(11, 4))
    ax.plot(an.index, an["count"], label="count")
    ax.plot(an.index, an["ewma"], label="ewma")

    alert_mask = an["alert"].fillna(False)
    ax.scatter(an.index[alert_mask], an.loc[alert_mask, "count"], marker="o", s=60, label="ALERT")

    ax.set_title("Issue count vs EWMA")
    ax.legend()
    fig.tight_layout()
    st.pyplot(fig)

    # st.markdown("**ìµœê·¼ 20ê°œ ë²„í‚·**")
    # st.dataframe(an.tail(20))
    st.markdown("**ì „ì²´ ë²„í‚· (CSV ì „ì²´ ê¸°ì¤€)**")
    st.dataframe(an, use_container_width=True)

    alerts_only = an[an["alert"].fillna(False)].copy()
    st.markdown("**ê°ì§€ëœ ALERT ëª©ë¡**")
    if alerts_only.empty:
        st.info("ê°ì§€ëœ ALERTê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        alerts_view = alerts_only[["count", "ewma", "zscore"]].assign(ts=alerts_only.index)
        st.dataframe(alerts_view.set_index("ts"))
        st.download_button(
            "ALERT CSV ë‹¤ìš´ë¡œë“œ",
            alerts_only.to_csv().encode("utf-8-sig"),
            file_name="alerts.csv",
            mime="text/csv",
        )

st.success("âœ… PoC ì™„ë£Œ: 3.1~3.4 ì „ì²´ ê¸°ëŠ¥ ì²´ì¸ì„ í•œ í™”ë©´ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
