"""
KoELECTRA ê¸°ë°˜ ê°ì • ë¶„ì„ ëª¨ë“ˆ
ì‚¬ì „í•™ìŠµëœ í•œêµ­ì–´ ELECTRA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ê°ì •(ë¶€ì •/ì¤‘ë¦½/ê¸ì •)ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.
"""
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import warnings
warnings.filterwarnings('ignore')

class KoElectraSentimentClassifier:
    """
    ì‚¬ì „í•™ìŠµëœ KoELECTRAë¥¼ ì‚¬ìš©í•œ ê°ì • ë¶„ì„ ë¶„ë¥˜ê¸°
    
    - ëª¨ë¸: monologg/koelectra-base-v3-discriminator (ì‚¬ì „í•™ìŠµ ëª¨ë¸)
    - ì¶œë ¥: ë¶€ì •(0), ì¤‘ë¦½(1), ê¸ì •(2)
    - ìš©ë„: ê²Œì„ ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€ì˜ ê°ì • ë¶„ì„
    """
    
    def __init__(self, model_name="./koelectra-game-sentiment", use_finetuned=True):
        """
        KoELECTRA ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            model_name: ëª¨ë¸ ê²½ë¡œ (ë¡œì»¬ ë˜ëŠ” Hugging Face)
            use_finetuned: Fine-tuned ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
        """
        # Fine-tuned ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        if use_finetuned:
            # Fine-tuned ëª¨ë¸ ê²½ë¡œ í™•ì¸
            import os
            if os.path.exists("./koelectra-game-sentiment"):
                model_name = "./koelectra-game-sentiment"
                print(f"ğŸŒŸ Fine-tuned KoELECTRA ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
            else:
                print(f"âš ï¸ Fine-tuned ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì‚¬ìš©")
                model_name = "monologg/koelectra-base-v3-discriminator"
                print(f"ğŸ¤– ì‚¬ì „í•™ìŠµ KoELECTRA ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        else:
            print(f"ğŸ¤– ì‚¬ì „í•™ìŠµ KoELECTRA ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
            self.model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            
            # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ, ì•„ë‹ˆë©´ CPU
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model.to(self.device)
            
            print(f"âœ… KoELECTRA ëª¨ë¸ ë¡œë”© ì™„ë£Œ (device: {self.device})")
            
        except Exception as e:
            print(f"âŒ KoELECTRA ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def predict_sentiment(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            "ë¶€ì •", "ì¤‘ë¦½", "ê¸ì •" ì¤‘ í•˜ë‚˜
        """
        if not text or not text.strip():
            return "ì¤‘ë¦½"
        
        try:
            # í† í°í™”
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            ).to(self.device)
            
            # ì˜ˆì¸¡ (gradient ê³„ì‚° ë¶ˆí•„ìš”)
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                pred = torch.argmax(logits, dim=1).item()
            
            # ë ˆì´ë¸” ë§¤í•‘
            # 0: ë¶€ì •, 1: ì¤‘ë¦½, 2: ê¸ì •
            labels = ["ë¶€ì •", "ì¤‘ë¦½", "ê¸ì •"]
            return labels[pred]
            
        except Exception as e:
            print(f"âš ï¸ ê°ì • ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ({text[:30]}...): {e}")
            return "ì¤‘ë¦½"  # ì˜¤ë¥˜ ì‹œ ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
    
    def predict_batch(self, texts: list) -> list:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì˜ˆì¸¡ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ)
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ê°ì • ë¦¬ìŠ¤íŠ¸ ["ë¶€ì •", "ì¤‘ë¦½", "ê¸ì •", ...]
        """
        if not texts:
            return []
        
        try:
            # ë°°ì¹˜ í† í°í™”
            inputs = self.tokenizer(
                texts,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            ).to(self.device)
            
            # ë°°ì¹˜ ì˜ˆì¸¡
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                preds = torch.argmax(logits, dim=1).cpu().numpy()
            
            # ë ˆì´ë¸” ë§¤í•‘
            labels = ["ë¶€ì •", "ì¤‘ë¦½", "ê¸ì •"]
            return [labels[p] for p in preds]
            
        except Exception as e:
            print(f"âš ï¸ ë°°ì¹˜ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê°œë³„ ì˜ˆì¸¡ìœ¼ë¡œ ëŒ€ì²´
            return [self.predict_sentiment(text) for text in texts]


# ì „ì—­ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_global_model = None

def get_sentiment_classifier():
    """
    ì „ì—­ KoELECTRA ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë”©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
    """
    global _global_model
    if _global_model is None:
        _global_model = KoElectraSentimentClassifier()
    return _global_model

